DESCRIPTION:
------------

###########################
##  GENERAL DESCRIPTION  ##
###########################

I have used 2 data sets for this Assignment , one was used for clustering/Community Detection , while the other
was used for classification of sentiment.

They are as follows:
--------------------

1) Community Detection of Ellon Musk's Followers and their followers .

2) Classification of Sentiment of tweets related to Donald Trump.

Classification of Sentiment of Tweets related to Donald Trump:
--------------------------------------------------------------
I collected 1000 tweets related to the term "Trump" and annotated each Tweet using Affin data set in collect.py.
In classify.py i read these saved tweets from the .csv file and cleaned the tweets by removing the stopwords using the
nltk stopwords corpus. I vectorized entire data and then split the vectorized data into train and test my csr matrices.
After this I use these matrices in my Support Vector Machine to classify them as either Positive Sentiment (Class Label : 1)
or Negative Sentiment (Class Label : 0).

Clustering of Followers of Ellon Musk:
--------------------------------------
I collected 200 followers of Ellon Musk, then i collected 200 more followers for each of the original 200 , to try and
identify the communities or the similarity between the people who follow Ellon Musk. After this i read the saved .json
file generated by collect.py in my cluster.py after which i generate a graph using the networkx library .

Graph is represented in this way:

1) Node : Each node is represented as a user , using his user_id

2) Edge : This represents the relationship between each Node , here i define the relationship to be
  -> if A follows B then edge AB exists
  -> if B follows A then still edge AB exists
  I have used a Undirected Graph for this.

After the graph Generation i save it before clustering as Graph.png, after which i cluster this graph using girvan_newman
graph clustering algorithm. After the clustering i save the cluster details as well as plot the graph with clusters, where
each color of a node in my graph represents a cluster. The girvan_newman algorithm is in networkx 2.0 package. I upgraded
this package and used the algorithm.

***********************************************************************************************************************

###########################
##  HOW TO EXECUTE CODE  ##
###########################

1) Run collect.py first before any of the other files to collect the data to classify and cluster.
   cmd: python collect.py

2) Then you can either run cluster.py or classify.py.
   cmd: python cluster.py
   cmd: python classify.py

3) Now after running all the 3 shown above , run summarize.py
   cmd: python summarize.py

4) To view the 'summary.txt' check the Summarize_Folder folder

5) To view the unclustered graph 'Graph.png' and clustered graph 'Clustered_Graph.png' check the Cluster_Folder folder.

6) To view the clusters of twitter user id's check the cluster.txt file in the Cluster_Folder folder.

***********************************************************************************************************************

###########################
##       ANALYSIS        ##
###########################

Clustering :
------------
After collecting the Ellon musk followers data, I used girvan newman algorithm for those followers who are following
Ellon Musk. After filtering the followers I draw a graph by adding edge between all followers and followed. I can see
communities of filtered followers before clustering. I created 5 clusters and the ripple effect if spread to one community,
easily spread to different communities of Ellon Musk. Hence it becomes easier for people who do marketing to analyze which
customer's to target in this way. If more data about each user was available then we could categorize each cluster's sex,
age-group and occupation they do then we could get more insights on what kind of people actually follow Ellon Musk and
what kind of people follow the followers of Ellon Musk. After clustering into 5 communities i created a colored graph for
representing the communities and the average number of people for each community is 1000.


Classification:
---------------
After collecting the 1000 tweets with 'Trump' as search word and annotated each tweet using Affin data set. But the main
problem here was the examples for positive class were very few in number compared to my negative class examples, even
after 4 days of continuous streaming and checking. Due to bias in positive and negative examples, my classifier predicted
most of positive examples as negative. We can overcome this problem by under-sampling the highest class or over-sampling the
lowest class members. I removed URL's, punctuations and [.,@$"!%-&+] characters, if I change my tokenizer function then
I might get more accuracy. Next time, I need to pay attention with tokenizer function also. But, overall 80% is a good
accuracy of prediction with 1000 tweets, and we can see that sentiment for Trump is more negative than positive. If I
increase the size of data it might help me to better analyze the results.

************************************************************************************************************************

IMPORTANT NOTE:
--------------
Due to twitter api's rate limmiting the process of collecting the followers takes a very long time,
hence if you do not want to wait that long then you need to change :

In collector.py , in main() method  line 230:
--> change users_count parameter in the method followers_map() to a lower value :
    if you put users_count =  x
    then you get back around x^2 id's in total.

Also in Graph clustering in cluster.py generating the clustered graph takes quite a while with large data and this
just increases for large cluster number , i have given the no of clusters to form as 7 as a parameter to my method
and this takes alot of time . When testing you can change this to a smaller number and check.

change line 239 in cluster.py :
--> in the method cluster_graph() change 'k' to a smaller value for faster runtime.

************************************************************************************************************************

REQUIREMENTS:
-------------
For my code to work these are the libraries that are required.
1) SKlearn
2) OS
3) TwitterApi
4) twitter                      (Streaming api : install using --> pip install twitter)
5) Nltk stopword corpus         (install using --> nltk download then pick the stopword corpus and install)
6) Faker                        (install using --> pip install faker)
7) Networkx
8) Itertools
9) Matplotlib
10)networkx.algorithms.community.centrality (girvan_newman algorithm)

************************************************************************************************************************
